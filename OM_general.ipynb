{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'nileshtrip'\n",
    "import numpy as np\n",
    "import math\n",
    "import subprocess\n",
    "import itertools\n",
    "import gc\n",
    "import copy\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import transpose as trans\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "\n",
    "import mkl\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import ray\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "subprocess.call(\"bash convert_files.sh\", shell=True)\n",
    "\n",
    "from auxiliary import is_pos_def, cond, rotate_matrix, gen_train_data, gen_test_data\n",
    "from datasets import load_parkinson, load_triazines,  load_wine, load_fertility, load_forest_fires, load_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunk_data(X_test, y_test, chunk_size):\n",
    "    \n",
    "    \"\"\"Divides X_test and y_test into list of chunks of dataset (in order to parallelize)\"\"\"\n",
    "    \n",
    "    num = X_test.shape[0]\n",
    "    num_chunks = math.floor(num/chunk_size)\n",
    "    print(\"Chunking\")\n",
    "    \n",
    "    chunks=[]\n",
    "    if num_chunks > 0:\n",
    "        for i in range(num_chunks):\n",
    "            a = i*chunk_size\n",
    "            b = (i+1)*chunk_size\n",
    "            chunks.append((X_test[a:b, :], y_test[a:b]))\n",
    "\n",
    "        chunks.append((X_test[b:, :], y_test[b:]))\n",
    "    else:\n",
    "        chunks.append((X_test, y_test))\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_lasso(train_data, sigma, cv, fit_intercept, alpha_scaling=1.0, n_folds=5):\n",
    "    \n",
    "    \"\"\" Lasso fitter. If cv True uses CV to fit; if false will use alpha_scaling * \\sqrt{2 log p/n} as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Lasso, LassoCV\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling*sigma*math.sqrt(2* (math.log(p)/n))\n",
    "        lasso = Lasso(alpha=alpha, max_iter=5000, fit_intercept=fit_intercept)\n",
    "        lasso.fit(X_train, y_train)\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        lasso=LassoCV(max_iter=5000, cv=n_folds, alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        lasso.fit(X_train, y_train)\n",
    "        \n",
    "    return lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_ridge(train_data, sigma, cv, fit_intercept, alpha_scaling=1.0):\n",
    "    \n",
    "    \"\"\" ridge regression fitter. If cv True uses CV to fit; if false will use alpha_scaling as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling\n",
    "        ridge = Ridge(alpha=alpha, fit_intercept=fit_intercept)\n",
    "        ridge.fit(X_train, y_train)\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        ridge=RidgeCV(alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        ridge.fit(X_train, y_train)\n",
    "\n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_elastic(train_data, sigma, cv, fit_intercept, alpha_scaling=1.0, n_folds=5):\n",
    "    \n",
    "    \"\"\" elastic net fitter. If cv True uses CV to fit; if false will use alpha_scaling as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "    \n",
    "    if not cv:\n",
    "        alpha = alpha_scaling*sigma*math.sqrt(2*(math.log(p)/n))\n",
    "        elastic = ElasticNet(alpha=alpha, max_iter=5000, fit_intercept=fit_intercept)\n",
    "        elastic.fit(X_train, y_train)\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        ratio = [.1, .5, .7, .9, .95, .99, 1]\n",
    "        elastic=ElasticNetCV(max_iter=5000, cv=n_folds, l1_ratio = ratio, alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        elastic.fit(X_train, y_train)\n",
    "\n",
    "    return elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_rf(train_data, n_estimators, random_state, max_depth):\n",
    "    \n",
    "    \"\"\" RF fitter. n_estimators indicates number of decision trees, and max_depth indicates max tree depth.\"\"\"\n",
    "   \n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state, max_depth=max_depth)  \n",
    "    regressor.fit(X_train, y_train)  \n",
    "\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_par_lasso(train_data, sigma, cv, fit_intercept, threads, alpha_scaling, n_folds=5, max_calls=1):\n",
    "\n",
    "    \"\"\" Parallel version of Lasso regression fitter. If cv True uses CV to fit; if false will use alpha_scaling * \\sqrt{2 log p/n} as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "    \n",
    "    mkl.set_num_threads(threads)\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Lasso, LassoCV\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling*sigma*math.sqrt(2* (math.log(p)/n))\n",
    "        lasso = Lasso(alpha=alpha, max_iter=5000, fit_intercept=fit_intercept)\n",
    "        lasso.fit(X_train.copy(), y_train.copy())\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        lasso=LassoCV(max_iter=5000, cv=n_folds, alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        lasso.fit(X_train.copy(), y_train.copy())\n",
    "        \n",
    "    return lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_par_ridge(train_data, sigma, cv, fit_intercept, threads, alpha_scaling=1.0, max_calls=1):\n",
    "    \n",
    "    \"\"\" Parallel version of ridge regression fitter. If cv True uses CV to fit; if false will use alpha_scaling as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "    \n",
    "    mkl.set_num_threads(threads) # set number of threads for function.\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling\n",
    "        ridge = Ridge(alpha=alpha, fit_intercept=fit_intercept)\n",
    "        ridge.fit(X_train.copy(), y_train.copy())\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        ridge=RidgeCV(alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        ridge.fit(X_train.copy(), y_train.copy()) #copy data since parallelizing\n",
    "        \n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_par_elastic(train_data, sigma, cv, fit_intercept, threads, alpha_scaling=1.0, n_folds=5, max_calls=1):\n",
    "    \n",
    "    \"\"\" Parallel version of elastic net fitter. If cv True uses CV to fit; if false will use alpha_scaling as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "    \n",
    "    \n",
    "    mkl.set_num_threads(threads)\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling*sigma*math.sqrt(2* (math.log(p)/n))\n",
    "        elastic = ElasticNet(alpha=alpha, max_iter=5000, fit_intercept=fit_intercept)\n",
    "        elastic.fit(X_train.copy(), y_train.copy())\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        ratio = [.1, .5, .7, .9, .95, .99, 1]\n",
    "        elastic=ElasticNetCV(max_iter=5000, cv=n_folds, l1_ratio = ratio, alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        elastic.fit(X_train.copy(), y_train.copy())\n",
    "        \n",
    "    return elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_f_lassos(train_data, sigma, cv, fit_intercept, n_splits, threads, alpha_scaling, n_folds=5):\n",
    "    \n",
    "    \"\"\"Fit all needed f lasso regressions needed for OM estimator when using n_splits-crossfitting\"\"\"\n",
    "\n",
    "    X, y = train_data\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    lassos=ray.get([fit_par_lasso.remote((X[train_index, :], y[train_index]), sigma, cv, fit_intercept, threads=threads, alpha_scaling=alpha_scaling, n_folds=n_folds) for train_index, test_index in kf.split(X)])\n",
    "    \n",
    "    return lassos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_f_ridges(train_data, sigma, cv, fit_intercept, n_splits, threads, alpha_scaling=1.0):\n",
    "\n",
    "    \"\"\"Fit all needed f ridge regressions needed for OM estimator when using n_splits-crossfitting\"\"\"\n",
    "    \n",
    "    X, y = train_data\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    ridges=ray.get([fit_par_ridge.remote((X[train_index, :], y[train_index]), sigma, cv, fit_intercept, threads=threads, alpha_scaling=alpha_scaling) for train_index, test_index in kf.split(X)])\n",
    "    \n",
    "    return ridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_f_elastics(train_data, sigma, cv, fit_intercept, n_splits, threads, alpha_scaling=1.0, n_folds=5):\n",
    "\n",
    "    \"\"\"Fit all needed f elastic net regressions needed for OM estimator when using n_splits-crossfitting\"\"\"\n",
    "        \n",
    "    X, y = train_data\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    elastics=ray.get([fit_par_elastic.remote((X[train_index, :], y[train_index]), sigma, cv, fit_intercept, threads=threads, alpha_scaling=alpha_scaling, n_folds=n_folds) for train_index, test_index in kf.split(X)])\n",
    "    \n",
    "    return elastics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_g_lasso(train_data, test_point, sigma, cv, fit_intercept, threads, n_splits, alpha_scaling, n_folds=5, max_calls=1):\n",
    "    \n",
    "    \"\"\"Fit lassos for auxiliary g regression \"\"\"\n",
    "    \n",
    "    mkl.set_num_threads(threads)\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X = train_data\n",
    "    n = X.shape[0]\n",
    "    kf = KFold(n_splits=n_splits)  \n",
    "    # builds (scaled) rotation matrix U\n",
    "\n",
    "    test_norm = np.linalg.norm(test_point, 2)\n",
    "    U = test_norm * rotate_matrix(test_point)\n",
    "    U_inv = np.linalg.inv(U)\n",
    "    \n",
    "    # construct rotated data matrix (which uses U_inv^\\top)\n",
    "    Xrot = X @ U_inv\n",
    "    # split features into \"T\" and remaining \"X\"\n",
    "    t, x = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "    model_ts = [fit_lasso((x[train_index, :], t[train_index]), sigma=sigma, cv=cv, fit_intercept=fit_intercept, alpha_scaling=alpha_scaling, n_folds=n_folds) for train_index, test_index in kf.split(X)]\n",
    "    \n",
    "    i=0\n",
    "    res_t=np.zeros(n)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        res_t[test_index] = t[test_index]-model_ts[i].predict(x[test_index, :])\n",
    "        i+=1\n",
    "    \n",
    "    return model_ts, res_t, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_g_lassos(train_data, test_data, sigma, cv, fit_intercept, threads, n_splits, alpha_scaling, n_folds=5):\n",
    "    \n",
    "    \"\"\"Fit all g lassos for auxiliary g regression \"\"\"\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X_train = train_data\n",
    "    X_test = test_data\n",
    "    num_train, p = X_train.shape\n",
    "    num_test, p = X_test.shape\n",
    "    \n",
    "    stuff = num_test * [None]\n",
    "\n",
    "    id_dict = {fit_g_lasso.remote(train_data=train_data, test_point=X_test[i, :], sigma=sigma, cv=cv, fit_intercept=fit_intercept, threads=threads, n_splits=n_splits, alpha_scaling=alpha_scaling) : i for i in range(num_test)}\n",
    "    ids = list(id_dict.keys())\n",
    "    old_ids = copy.deepcopy(ids)\n",
    "    \n",
    "    while len(ids) > 0:\n",
    "        ready_ids, ids = ray.wait(ids)\n",
    "        for ready_id in ready_ids:\n",
    "            stuff[id_dict[ready_id]] = ray.get(ready_id)\n",
    "    ray.internal.free(old_ids)\n",
    "            \n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_g_rf(train_data, test_point, n_splits, n_estimators, random_state, max_depth, threads, max_calls=1):\n",
    "   \n",
    "    \"\"\"Fit g RFs for auxiliary g regression \"\"\"\n",
    "    \n",
    "    \n",
    "    mkl.set_num_threads(threads)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    X = train_data\n",
    "    n = X.shape[0]\n",
    "    kf = KFold(n_splits=n_splits)  \n",
    "    # builds (scaled) rotation matrix U\n",
    "\n",
    "    test_norm = np.linalg.norm(test_point, 2)\n",
    "    U = test_norm * rotate_matrix(test_point)\n",
    "    U_inv = np.linalg.inv(U)\n",
    "    \n",
    "    # construct rotated data matrix (which uses U_inv^\\top)\n",
    "    Xrot = X @ U_inv\n",
    "    # split features into \"T\" and remaining \"X\"\n",
    "    t, x = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "    model_ts = [fit_rf((x[train_index, :], t[train_index]), n_estimators, random_state, max_depth) for train_index, test_index in kf.split(X)]\n",
    "    \n",
    "    i=0\n",
    "    res_t=np.zeros(n)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        res_t[test_index] = t[test_index]-model_ts[i].predict(x[test_index, :])\n",
    "        i+=1\n",
    "        \n",
    "    return model_ts, res_t, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_g_rfs(train_data, test_data, n_splits, n_estimators, threads, max_depth, random_state):\n",
    "    \n",
    "    \"\"\"Fit all g RFs for auxiliary g regression \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X_train = train_data\n",
    "    X_test = test_data\n",
    "    num_train, p = X_train.shape\n",
    "    num_test, p = X_test.shape\n",
    "    \n",
    "    stuff = num_test * [None]\n",
    "\n",
    "    id_dict = {fit_g_rf.remote(train_data=train_data, test_point=X_test[i, :], n_splits=n_splits, n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, threads=threads) : i for i in range(num_test)}\n",
    "    ids = list(id_dict.keys())\n",
    "    old_ids = copy.deepcopy(ids)\n",
    "    \n",
    "    while len(ids) > 0:\n",
    "        ready_ids, ids = ray.wait(ids)\n",
    "        for ready_id in ready_ids:\n",
    "            stuff[id_dict[ready_id]] = ray.get(ready_id)\n",
    "            \n",
    "    ray.internal.free(old_ids)\n",
    "            \n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_g_zeros(X_train, X_test, n_splits):\n",
    "    \n",
    "    \"\"\"Fit all g 0s for auxiliary g regression \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    num_train, p = X_train.shape\n",
    "    num_test, _ = X_test.shape\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)  \n",
    "    # builds (scaled) rotation matrix U\n",
    "\n",
    "    all_models=[]\n",
    "    for i in range(X_test.shape[0]):\n",
    "        test_point = X_test[i, :]\n",
    "        test_norm = np.linalg.norm(test_point, 2)\n",
    "        U = test_norm * rotate_matrix(test_point)\n",
    "        U_inv = np.linalg.inv(U)\n",
    "\n",
    "        # construct rotated data matrix (which uses U_inv^\\top)\n",
    "        Xrot = X_train @ U_inv\n",
    "        # split features into \"T\" and remaining \"X\"\n",
    "        t, x = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "        j=0\n",
    "        model_ts_new=[]\n",
    "        for train_index, test_index in kf.split(X_train):\n",
    "            model_ts_new.append(None)\n",
    "            j+=1\n",
    "        all_models.append((model_ts_new, t, t))\n",
    "            \n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote \n",
    "def fit_f_moms_1f(data, sigma, test_point, g_bases, f_bases, cv, fit_intercept, n_splits, threads):\n",
    "    \n",
    "    \"\"\"Parallel function to predict y values using the OM f moments. Makes a single prediction on test_point x_* using OM \n",
    "    by rotating all data into the basis where test_point points along e_1. \n",
    "    \n",
    "    Uses (y-\\theta t - z^\\top f) (t-g(z)) moments. Here f(z) is estimated by regressing y on (t,z) and dropping t.\n",
    "    \n",
    "    # data is training data\n",
    "    # test_point is the prediction direction\n",
    "    # sigma is the additive noise\n",
    "    # alpha1 and alpha2 represent the regularization scaling for both lasso regressions respectively needed for prediction\n",
    "    # cv indicates whether or not to use CV\n",
    "    # fit_intercept indicates whether or not to fit y-intercepts\n",
    "    \"\"\"\n",
    "    mkl.set_num_threads(threads) # set number of threads for function\n",
    "\n",
    "    # builds (scaled) rotation matrix U\n",
    "    X, y = data\n",
    "    test_norm = np.linalg.norm(test_point, 2)\n",
    "    U = test_norm * rotate_matrix(test_point)\n",
    "    U_inv = np.linalg.inv(U)\n",
    "    \n",
    "    # construct rotated data matrix (which uses U_inv^\\top)\n",
    "    Xrot = X @ U_inv\n",
    "    # split features into \"T\" and remaining \"X\"\n",
    "    Trot, Xrot = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "    # arrays to contain p and q residuals from regression \n",
    "    res_t = np.zeros(X.shape[0])\n",
    "    res_y = np.zeros(X.shape[0])\n",
    "\n",
    "    kf = KFold(n_splits=n_splits) #first stage CF\n",
    "    \n",
    "    count=0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data in half, train and test\n",
    "        x_train, t_train, y_train = Xrot[train_index, :], Trot[train_index], y[train_index]\n",
    "        x_test, t_test, y_test  = Xrot[test_index, :], Trot[test_index], y[test_index]\n",
    "        \n",
    "        # Fit Lasso models for \"y\" in original basis\n",
    "        model_y_orig = f_bases[count]\n",
    "        y_coef = model_y_orig.coef_\n",
    "        y_int = model_y_orig.intercept_\n",
    "        \n",
    "        # Rotate \"y\" coefficients (i.e. f) into new basis and \"drop\" first t coefficient\n",
    "        y_coef_rot = (U @ y_coef)[1:]\n",
    "\n",
    "        model_t = g_bases[count]\n",
    "        if model_t==None:\n",
    "            res_t[test_index] = (t_test - 0).flatten()\n",
    "        else:\n",
    "            res_t[test_index] = (t_test - model_t.predict(x_test)).flatten()\n",
    "\n",
    "        res_y[test_index] = (y_test - (x_test @ y_coef_rot + y_int)).flatten()\n",
    "        count+=1\n",
    "\n",
    "    num1 = np.mean(res_y * res_t)\n",
    "    denom1 = np.mean(Trot * res_t)\n",
    "    pred1 = num1 / denom1\n",
    "    \n",
    "    return pred1, num1, denom1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote \n",
    "def fit_q_moms(data, sigma, test_point, g_bases, f_method, cv, fit_intercept, n_splits, threads):\n",
    "    \n",
    "    \"\"\"Parallel function to predict y values using the OM q moments. Makes a single prediction on test_point x_* using OM \n",
    "    by rotating all data into the basis where test_point points along e_1. \n",
    "    \n",
    "    Uses (y-q(z)-theta(t-g(z)))(t-g(z)) moments. Here q(z) is estimated by regressing y on z and then directly regressing t on z.\n",
    "    \n",
    "    # data is training data\n",
    "    # test_point is the prediction direction\n",
    "    # sigma is the additive noise\n",
    "    # alpha1 and alpha2 represent the regularization scaling for both lasso regressions respectively needed for prediction\n",
    "    # cv indicates whether or not to use CV\n",
    "    # fit_intercept indicates whether or not to fit y-intercepts\n",
    "    \"\"\"\n",
    "    mkl.set_num_threads(threads)\n",
    "        \n",
    "    from sklearn.linear_model import LassoCV\n",
    "\n",
    "    # builds (scaled) rotation matrix U\n",
    "    X, y = data\n",
    "    test_norm = np.linalg.norm(test_point, 2)\n",
    "    U = test_norm * rotate_matrix(test_point)\n",
    "    U_inv = np.linalg.inv(U)\n",
    "\n",
    "    # construct rotated data matrix (which uses U_inv^\\top)\n",
    "    Xrot = X @ U_inv\n",
    "    # split features into \"T\" and remaining \"X\"\n",
    "    Trot, Xrot = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "    # arrays to contain p and q residuals from regression (*remainder of code similar to OML code paper)\n",
    "    res_t = np.zeros(X.shape[0])\n",
    "    res_y = np.zeros(X.shape[0])\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    #first stage CF\n",
    "    count=0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data in half, train and test\n",
    "        x_train, t_train, y_train = Xrot[train_index, :], Trot[train_index], y[train_index]\n",
    "        x_test, t_test, y_test  = Xrot[test_index, :], Trot[test_index], y[test_index]\n",
    "        \n",
    "        # Fit with Lassso the treatment as a function of x and the outcome as\n",
    "        # a function of x, using only the train fold\n",
    "        if f_method==\"Lasso\":\n",
    "            model_y = fit_lasso((x_train, y_train), sigma, cv=cv, fit_intercept=fit_intercept)\n",
    "        elif f_method==\"Ridge\":\n",
    "            model_y = fit_ridge((x_train, y_train), sigma, cv=cv, fit_intercept=fit_intercept)\n",
    "        elif f_method==\"Elastic\":\n",
    "            model_y = fit_elastic((x_train, y_train), sigma, cv=cv, fit_intercept=fit_intercept)\n",
    "        else:\n",
    "            raise Exception(\"Main Set Incorrectly\")\n",
    "        # Then compute residuals t-g(x) and y-z(x) on test fold\n",
    "        \n",
    "        model_t = g_bases[count]\n",
    "        if model_t==None:\n",
    "            res_t[test_index] = (t_test - 0).flatten()\n",
    "        else:\n",
    "            res_t[test_index] = (t_test - model_t.predict(x_test)).flatten()\n",
    "\n",
    "        res_y[test_index] = (y_test - model_y.predict(x_test)).flatten()\n",
    "        count+=1\n",
    "        \n",
    "    num1 = np.mean(res_y * res_t)\n",
    "    denom1 = np.mean(res_t * res_t)\n",
    "    pred1 = num1 / denom1\n",
    "    \n",
    "    return pred1, num1, denom1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_f_moms_1f(train_data, X_test, g_bases, f_bases, sigma, cv, fit_intercept, n_splits, threads):\n",
    "\n",
    "    \"\"\" Make predictions over entire test set in X_test using f_bases, g_bases which have already been fit. \"\"\"\n",
    "    \n",
    "    X_train, y_train = train_data \n",
    "    _, p = X_train.shape\n",
    "    \n",
    "    num_test,_ = X_test.shape\n",
    "    stuff = num_test * [None]\n",
    "    \n",
    "    # Function to make predictions over test set in parallel\n",
    "    id_dict = {fit_f_moms_1f.remote(train_data, sigma, X_test[i, :], g_bases[i], f_bases, cv=cv, fit_intercept=fit_intercept, n_splits=n_splits, threads=threads) : i for i in range(num_test)}\n",
    "    ids = list(id_dict.keys())\n",
    "    old_ids = copy.deepcopy(ids)\n",
    "    \n",
    "    while len(ids) > 0:\n",
    "        ready_ids, ids = ray.wait(ids)\n",
    "        for ready_id in ready_ids:\n",
    "            stuff[id_dict[ready_id]] = ray.get(ready_id)\n",
    "    \n",
    "    pred1s, num1s, denom1s = zip(*stuff)\n",
    "    ray.internal.free(old_ids)\n",
    "\n",
    "    return (np.array(pred1s), np.array(num1s), np.array(denom1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_q_moms(train_data, X_test, g_bases, f_method, sigma, cv, fit_intercept, n_splits, threads):\n",
    "\n",
    "    \"\"\" Make predictions over entire test set in X_test using f_method, g_bases which have already been fit. \"\"\"\n",
    "        \n",
    "    X_train, y_train = train_data \n",
    "    _, p = X_train.shape\n",
    "    \n",
    "    num_test,_ = X_test.shape\n",
    "    stuff = num_test * [None]\n",
    "    \n",
    "    id_dict = {fit_q_moms.remote(train_data, sigma, X_test[i, :], g_bases[i], f_method, cv=cv, fit_intercept=fit_intercept, n_splits=n_splits, threads=threads): i for i in range(num_test)}\n",
    "    ids = list(id_dict.keys())\n",
    "    old_ids = copy.deepcopy(ids)\n",
    "    \n",
    "    while len(ids) > 0:\n",
    "        ready_ids, ids = ray.wait(ids)\n",
    "        for ready_id in ready_ids:\n",
    "            stuff[id_dict[ready_id]] = ray.get(ready_id)\n",
    "    \n",
    "    pred1s, num1s, denom1s = zip(*stuff)\n",
    "    ray.internal.free(old_ids)\n",
    "\n",
    "    return (np.array(pred1s), np.array(num1s), np.array(denom1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asymp_var_q(res_t):\n",
    "    \n",
    "    denom = (np.mean(res_t * res_t))**2\n",
    "    num = np.var(res_t)\n",
    "    \n",
    "    var = num/denom\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asymp_var_f(res_t, t):\n",
    "    \n",
    "    denom = (np.mean(res_t * t))**2\n",
    "    num = np.var(res_t)\n",
    "    \n",
    "    var = num/denom\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best(g_lassos, g_rfs1, g_zeros, n_splits, num_test, aux):\n",
    "    \n",
    "    best_method=[None for i in range(num_test)]\n",
    "    best_model = [[None for i in range(n_splits)] for i in range(num_test)]\n",
    "    for i in range(num_test):\n",
    "        if aux==\"q\":\n",
    "            lasso_var = asymp_var_q(g_lassos[i][1])\n",
    "            rf1_var = asymp_var_q(g_rfs1[i][1])\n",
    "            zeros_var = asymp_var_q(g_zeros[i][1])\n",
    "        elif aux==\"f\":\n",
    "            lasso_var = asymp_var_f(g_lassos[i][1], g_lassos[i][2])\n",
    "            rf1_var = asymp_var_f(g_rfs1[i][1], g_rfs1[i][2])\n",
    "            zeros_var = asymp_var_f(g_zeros[i][1], g_zeros[i][2])\n",
    "        for j in range(n_splits):\n",
    "            if lasso_var < rf1_var and lasso_var < zeros_var:\n",
    "                best_model[i][j] = g_lassos[i][0][j]\n",
    "                best_method[i] = \"Lasso\"\n",
    "            elif rf1_var < lasso_var and rf1_var < zeros_var:\n",
    "                best_model[i][j] = g_rfs1[i][0][j]\n",
    "                best_method[i] = \"RF_big\"\n",
    "            else:\n",
    "                best_model[i][j] = g_zeros[i][0][j]\n",
    "                best_method[i] = \"0\"\n",
    "                \n",
    "    return best_model, best_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_expt(data, cv, fit_intercept, n_splits, sigma, main_reg_params, aux_reg_params, parallel_params, seed):\n",
    "    # run an entire experiment for a given value of p, n, s. These will not use CV \n",
    "\n",
    "    chunk_size, threads = parallel_params\n",
    "    fit_intercept_main, fit_intercept_g = fit_intercept\n",
    "    np.random.seed(seed)\n",
    "    X_train, y_train, X_test, y_test = data\n",
    "    aux_reg = aux_reg_params[\"method\"]\n",
    "    main_reg = main_reg_params[\"method\"]\n",
    "    \n",
    "    if main_reg==\"Lasso\":\n",
    "        f_main = fit_all_f_lassos((X_train, y_train), sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads, alpha_scaling=1.0, n_folds=5)\n",
    "    elif main_reg==\"Ridge\":\n",
    "        f_main = fit_all_f_ridges((X_train, y_train), sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads, alpha_scaling=1.0)\n",
    "    elif main_reg==\"Elastic\":\n",
    "        f_main = fit_all_f_elastics((X_train, y_train), sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads,  alpha_scaling=1.0, n_folds=5)\n",
    "    else:\n",
    "        raise Exception(\"Main Reg Set Incorrectly\")    \n",
    "\n",
    "    if aux_reg==\"Best\":\n",
    "        g_lassos = fit_all_g_lassos(X_train, test_data=X_test, sigma=sigma, cv=cv, fit_intercept=fit_intercept_g, n_splits=n_splits, threads=threads, alpha_scaling=1.0, n_folds=5)\n",
    "        random_state1 = aux_reg_params[\"random_state1\"]\n",
    "        n_estimators1 = aux_reg_params[\"n_estimators1\"]\n",
    "        max_depth1 = aux_reg_params[\"max_depth1\"]\n",
    "        \n",
    "        g_rfs1 = fit_all_g_rfs(X_train, test_data=X_test, n_splits=n_splits, n_estimators=n_estimators1, max_depth=max_depth1, random_state=random_state1, threads=threads)\n",
    "        g_zeros = fit_all_g_zeros(X_train, X_test, n_splits)\n",
    "        best_gs_f, best_method_f = get_best(g_lassos, g_rfs1, g_zeros, n_splits, X_test.shape[0], aux=\"f\")\n",
    "        best_gs_q, best_method_q = get_best(g_lassos, g_rfs1, g_zeros, n_splits, X_test.shape[0], aux=\"q\")\n",
    "    elif aux_reg==\"0\":\n",
    "        best_gs = [[None for i in range(n_splits)] for i in range(X_test.shape[0])]\n",
    "    else:\n",
    "        raise Exception(\"Aux Reg Set Incorrectly\")\n",
    "\n",
    "    fo1f = fit_all_f_moms_1f(train_data=(X_train, y_train), X_test=X_test, g_bases=best_gs_f, f_bases=f_main, sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads)\n",
    "    foq = fit_all_q_moms(train_data=(X_train, y_train), X_test=X_test, g_bases=best_gs_q, f_method=main_reg, sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads)\n",
    "\n",
    "    if main_reg==\"Lasso\":\n",
    "        lasso = fit_lasso((X_train, y_train), sigma, alpha_scaling=1, cv=True, fit_intercept=fit_intercept_main)\n",
    "        base_preds = lasso.predict(X_test)\n",
    "    elif main_reg==\"Ridge\":\n",
    "        ridge = fit_ridge((X_train, y_train), sigma, alpha_scaling=1, cv=True, fit_intercept=fit_intercept_main)\n",
    "        base_preds = ridge.predict(X_test)\n",
    "    elif main_reg==\"Elastic\":\n",
    "        elastic = fit_elastic((X_train, y_train), sigma, alpha_scaling=1, cv=True, fit_intercept=fit_intercept_main)\n",
    "        base_preds = elastic.predict(X_test)\n",
    "    else:\n",
    "        raise Exception(\"Baseline Set Incorrectly\")\n",
    "        \n",
    "    kappa=cond(X_train)\n",
    "    #del(best_gs)\n",
    "    del(f_main)\n",
    "    gc.collect()\n",
    "    \n",
    "    return fo1f, foq, base_preds, y_test, kappa, best_method_f, best_method_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_expts(dataset, cv, fit_intercept, n_splits, sigma, main_reg_params, aux_reg_params, seed, folder_path, parallel_params):\n",
    "\n",
    "    # function to potentially parallelize experiments across various values of p, n, s and save data in pkl file\n",
    "    # can use python package ray to parallelize but numpy already parallelizes a lot.\n",
    "    save_data = OrderedDict()\n",
    "    \n",
    "    chunk_size, threads = parallel_params\n",
    "    fit_intercept_main = fit_intercept[0]\n",
    "    fit_intercept_g = fit_intercept[1]\n",
    "    save_data[\"dataset\"]=str(dataset)\n",
    "    save_data[\"best_g_method\"]=str(\"Minimize Asymptotic Variance Heuristic\")\n",
    "    save_data[\"method\"]=\"f moments and q moments\"\n",
    "    save_data[\"main_reg_params\"]=main_reg_params\n",
    "    save_data[\"fit_intercepts_main\"]=str(fit_intercept_main)\n",
    "    save_data[\"fit_intercepts_g\"]=str(fit_intercept_g)\n",
    "    save_data[\"format_keys\"] = \"p,n,s\"\n",
    "    save_data[\"output\"] = \"fo1f, foq, lasso_preds\" \n",
    "    save_data[\"n_splits\"] = n_splits\n",
    "    save_data[\"aux_reg_params\"] = aux_reg_params\n",
    "    save_data[\"seed\"] = seed\n",
    "    save_data[\"scale_X\"] = True\n",
    "    save_data[\"sigma\"] = sigma\n",
    "\n",
    "    if dataset==\"Triazines\":\n",
    "        data=load_triazines(test_size=.20, random_state=seed)\n",
    "    elif dataset==\"Wine\":\n",
    "        data=load_wine()\n",
    "    elif dataset==\"Parkinson\":\n",
    "        data=load_parkinson()\n",
    "    elif dataset==\"Fertility\":\n",
    "        data=load_fertility()\n",
    "    elif dataset==\"Fire\":\n",
    "        data=load_forest_fires()\n",
    "    elif dataset==\"Weather\":\n",
    "        data=load_weather()\n",
    "    \n",
    "    print(\"Starting\")\n",
    "    ## De-Meaning y values (X_train and X_test values have already been scaled in loader). Need to add back mu_y to all final predictions.\n",
    "    ## fit_intercept should be set to false\n",
    "    assert(fit_intercept_g==False, \"This should be set to false for all f regression. g_lasso has been manually set to include this\")\n",
    "    X_train, y_train, X_test, y_test = data\n",
    "    mu_y = np.mean(y_train)\n",
    "    y_train = y_train - mu_y\n",
    "    \n",
    "    chunk_tests = chunk_data(X_test, y_test, chunk_size)\n",
    "    all_chunks = [(X_train, y_train, chunk[0], chunk[1]) for chunk in chunk_tests]\n",
    "    \n",
    "    OM_fo1f_preds=[]\n",
    "    OM_fo1f_nums=[]\n",
    "    OM_fo1f_denoms=[]\n",
    "    \n",
    "    OM_foq_preds=[]\n",
    "    OM_foq_nums=[]\n",
    "    OM_foq_denoms=[]\n",
    "    \n",
    "    main_preds=[]\n",
    "    y_tests=[]\n",
    "    kappas=[]\n",
    "    best_methods_f=[]\n",
    "    best_methods_q=[]\n",
    "\n",
    "    flatten = lambda l: np.concatenate(l).ravel()\n",
    "    chunk_count=0\n",
    "    for chunk in all_chunks:\n",
    "        print(\"Chunk Count\", chunk_count)\n",
    "        fo1f, foq, main_pred, y_test, kappa, best_method_f, best_method_q = run_expt(data=chunk, cv=cv, fit_intercept=fit_intercept, n_splits=n_splits, sigma=sigma, main_reg_params=main_reg_params, \n",
    "                                                                                                               aux_reg_params=aux_reg_params, parallel_params=parallel_params, seed=seed)\n",
    "        main_reg=main_reg_params[\"method\"]\n",
    "        \n",
    "        OM_fo1f_preds.append(fo1f[0])\n",
    "        OM_fo1f_nums.append(fo1f[1])\n",
    "        OM_fo1f_denoms.append(fo1f[2])\n",
    "\n",
    "        OM_foq_preds.append(foq[0])\n",
    "        OM_foq_nums.append(foq[1])\n",
    "        OM_foq_denoms.append(foq[2])\n",
    "     \n",
    "        main_preds.append(main_pred)\n",
    "        y_tests.append(y_test)\n",
    "        kappas.append(kappa)\n",
    "        best_methods_f.append(best_method_f)\n",
    "        best_methods_q.append(best_method_q)\n",
    "        chunk_count+=1\n",
    "        gc.collect()\n",
    "\n",
    "    # IMPORTANT: y-values have been demeaned in the beginning. Need to add this back to ALL predictions.\n",
    "    OM_fo1f_preds= flatten(OM_fo1f_preds)+mu_y\n",
    "    OM_fo1f_nums= flatten(OM_fo1f_nums)\n",
    "    OM_fo1f_denoms= flatten(OM_fo1f_denoms)\n",
    "\n",
    "    OM_foq_preds= flatten(OM_foq_preds)+mu_y\n",
    "    OM_foq_nums= flatten(OM_foq_nums)\n",
    "    OM_foq_denoms= flatten(OM_foq_denoms)\n",
    "\n",
    "    main_preds= flatten(main_preds)+mu_y\n",
    "    y_tests= flatten(y_tests)\n",
    "    best_methods_f = [item for sublist in best_methods_f for item in sublist]\n",
    "    best_methods_q = [item for sublist in best_methods_q for item in sublist]\n",
    "    kappas= kappas[-1]\n",
    "    \n",
    "    save_data[\"results\"] = [\n",
    "            OM_fo1f_preds,\n",
    "            OM_fo1f_nums,\n",
    "            OM_fo1f_denoms,\n",
    "            OM_foq_preds,\n",
    "            OM_foq_nums,\n",
    "            OM_foq_denoms,\n",
    "            main_preds,\n",
    "            y_tests,\n",
    "            kappas,\n",
    "            best_methods_f,\n",
    "            best_methods_q]\n",
    "\n",
    "    print(\"Saving Data\")\n",
    "    time.sleep(1)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    aux_reg = str(aux_reg_params[\"method\"])\n",
    "    file_name = \"OM_\"+str(dataset)+\"_\"+str(aux_reg)+\"_\"+timestr+\".pickle\"\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    pickle.dump(save_data, open(file_path, \"wb\"))\n",
    "    \n",
    "    return save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=100\n",
    "scaling_params = [1.0]\n",
    "splits = [10]\n",
    "datasets=[\"Triazines\", \"Fire\", \"Fertility\", \"Wine\", \"Parkinson\"]\n",
    "\n",
    "\n",
    "cvs=[True]\n",
    "computer_cpus = 48\n",
    "main_reg_params = [({\"method\" : \"Lasso\"}, 48, computer_cpus//48), ({\"method\" : \"Ridge\"}, 12, computer_cpus//12), ({\"method\" : \"Elastic\"}, 48, computer_cpus//48)]\n",
    "aux_reg_params = [{\"method\" : \"Best\", \"n_estimators1\" : 50, \"random_state1\" : seed, \"max_depth1\" : None}]\n",
    "fit_intercept=(False, False)\n",
    "\n",
    "path_options=\"Weather\"\n",
    "now = datetime.datetime.now()\n",
    "folder_path=str(now.month)+\"-\"+str(now.day)+\"-\"+path_options\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ray.init(object_store_memory=int(6e10), num_cpus=48, redis_password=\"password34623345234\")\n",
    "ray.register_custom_serializer(sklearn.tree._tree.Tree, use_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total=0\n",
    "for aux_reg_param in aux_reg_params:\n",
    "    for main_reg_param in main_reg_params:\n",
    "        for split in splits:\n",
    "            for sigma in scaling_params:\n",
    "                for data in datasets:\n",
    "                    total+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for main_reg_param, chunk_size, threads in main_reg_params:\n",
    "    for aux_reg_param in aux_reg_params:\n",
    "        for cv in cvs:\n",
    "            for split in splits:\n",
    "                for sigma in scaling_params:\n",
    "                    for dataset in datasets:\n",
    "                        n_split = split\n",
    "                        print(\"{0:.0%}\".format(float(count)/total)+\" Done\")\n",
    "                        data = save_expts(dataset=dataset, cv=cv, fit_intercept=fit_intercept, n_splits=n_split, sigma=1.0, main_reg_params=main_reg_param,aux_reg_params=aux_reg_param, seed=seed, folder_path=folder_path, parallel_params=(chunk_size, threads))\n",
    "                        count+=1              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
