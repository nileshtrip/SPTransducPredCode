{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'nileshtrip'\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from collections import OrderedDict, defaultdict\n",
    "from scipy.stats import sem\n",
    "from datasets import load_wine, load_parkinson, load_triazines, load_fertility, load_forest_fires\n",
    "import csv\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncate(n, dig=0):\n",
    "    multiplier = 10 ** dig\n",
    "    return int(n * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def err_sd(err):\n",
    "    \n",
    "    err = np.array(err)\n",
    "    squared_err = np.power(err, 2)\n",
    "    se = np.std(squared_err)/np.sqrt(squared_err.shape[0])\n",
    "    \n",
    "    mu = np.mean(squared_err)\n",
    "    \n",
    "    return mu, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r_err_sd(err):\n",
    "    \n",
    "    err = np.array(err)\n",
    "    squared_err = np.power(err, 2)\n",
    "    se = np.std(squared_err)/np.sqrt(squared_err.shape[0])\n",
    "    \n",
    "    r_err = np.sqrt(np.mean(squared_err))\n",
    "    r_sd = 0.5*se/r_err\n",
    "    \n",
    "    return r_err, r_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r_err(err):\n",
    "    \n",
    "    err = np.array(err)\n",
    "    squared_err = np.power(err, 2)\n",
    "    mu = np.sqrt(np.mean(squared_err))\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_TDLasso_data(folder_path):\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    for data_file in [i for i in os.listdir(folder_path) if \".pickle\" in i]:\n",
    "        data = pickle.load(open(os.path.join(folder_path, data_file), \"rb\")) \n",
    "        for key in data.keys():\n",
    "            if key==\"results\":\n",
    "                #print(len(data[key]))\n",
    "                \n",
    "                tdlasso=np.array(data[key][0])\n",
    "                truth=np.array(data[key][1])\n",
    "                \n",
    "                #print(data[\"dataset\"])\n",
    "                #print(r_err_sd2(linreg-truth))\n",
    "                \n",
    "                errors_tdlasso = r_err_sd(np.ravel(tdlasso-truth))\n",
    "                results[(data[\"dataset\"], \"tdlasso\")] = errors_tdlasso\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdlasso_folder_path=\"4-11-TDLassoSP_Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(('Parkinson', 'tdlasso'),\n",
       "              (12.253520715326323, 0.13575597839123998)),\n",
       "             (('Fertility', 'tdlasso'),\n",
       "              (0.40920561422032503, 0.07162378610640245)),\n",
       "             (('Wine', 'tdlasso'), (0.9812686200048398, 0.015428162944273006)),\n",
       "             (('Fire', 'tdlasso'), (82.06568534308094, 36.032016265403165)),\n",
       "             (('Triazines', 'tdlasso'),\n",
       "              (0.14830938639979402, 0.023715140936050174))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_TDLasso_data(tdlasso_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_TDValsReg_data(folder_path):\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    for data_file in [i for i in os.listdir(folder_path) if \".pickle\" in i]:\n",
    "        data = pickle.load(open(os.path.join(folder_path, data_file), \"rb\")) \n",
    "        for key in data.keys():\n",
    "            if key==\"results\":\n",
    "                #print(len(data[key]))\n",
    "                \n",
    "                tdregeuclid=np.array(data[key][0])\n",
    "                truth=np.array(data[key][1])\n",
    "                \n",
    "                #print(data[\"dataset\"])\n",
    "                #print(r_err_sd2(linreg-truth))\n",
    "                \n",
    "                errors_tdvalsreg = r_err_sd(np.ravel(tdregeuclid-truth))\n",
    "                results[(data[\"dataset\"], \"tdvalsreg\")] = errors_tdvalsreg\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdvalsreg_folder_path=\"4-11-TDValsSP_Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(('Triazines', 'tdvalsreg'),\n",
       "              (0.173469200900365, 0.0036813964392945664)),\n",
       "             (('Parkinson', 'tdvalsreg'),\n",
       "              (12.25343578765015, 0.0021470346917246497)),\n",
       "             (('Fire', 'tdvalsreg'), (82.06635418216294, 2.5670262337393104)),\n",
       "             (('Wine', 'tdvalsreg'),\n",
       "              (0.8410590832690957, 0.0003736788904566116)),\n",
       "             (('Fertility', 'tdvalsreg'),\n",
       "              (0.4088802709453759, 0.01283775108227386))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_TDValsReg_data(tdvalsreg_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_TDRegKernel_data(folder_path):\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    for data_file in [i for i in os.listdir(folder_path) if \".pickle\" in i]:\n",
    "        data = pickle.load(open(os.path.join(folder_path, data_file), \"rb\")) \n",
    "        for key in data.keys():\n",
    "            if key==\"results\":\n",
    "                #print(len(data[key]))\n",
    "                \n",
    "                tdregkernel=np.array(data[key][0])\n",
    "                truth=np.array(data[key][1])\n",
    "                \n",
    "                #print(data[\"dataset\"])\n",
    "                #print(r_err_sd2(linreg-truth))\n",
    "                \n",
    "                errors_tdregkernel = r_err_sd(np.ravel(tdregkernel-truth))\n",
    "                results[(data[\"dataset\"], \"tdregkernel\")] = errors_tdregkernel\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdregkernel_folder_path=\"4-11-TDRegKernelSP_Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(('Parkinson', 'tdregkernel'),\n",
       "              (12.33263377313558, 0.14467599399723352)),\n",
       "             (('Triazines', 'tdregkernel'),\n",
       "              (0.15103470294183405, 0.024037624569851517)),\n",
       "             (('Fertility', 'tdregkernel'),\n",
       "              (0.3844995854289188, 0.07598235810001784)),\n",
       "             (('Wine', 'tdregkernel'),\n",
       "              (0.834501944324581, 0.01529846011068214)),\n",
       "             (('Fire', 'tdregkernel'),\n",
       "              (81.94670504805939, 35.83402534128768))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_TDRegKernel_data(tdregkernel_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_linreg_data(folder_path):\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    for data_file in [i for i in os.listdir(folder_path) if \".pickle\" in i]:\n",
    "        data = pickle.load(open(os.path.join(folder_path, data_file), \"rb\")) \n",
    "        for key in data.keys():\n",
    "            if key==\"results\":\n",
    "                #print(len(data[key]))\n",
    "                \n",
    "                linreg=np.array(data[key][0])\n",
    "                truth=np.array(data[key][1])\n",
    "                \n",
    "                #print(data[\"dataset\"])\n",
    "                #print(r_err_sd2(linreg-truth))\n",
    "                \n",
    "                errors_linreg = r_err_sd(np.ravel(linreg-truth))\n",
    "                results[(data[\"dataset\"], \"OLS\")] = errors_linreg\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols_folder_path=\"7-28-LinReg_Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_OLS(ols_folder_path):\n",
    "    \n",
    "    return list(get_linreg_data(folder_path = ols_folder_path).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OLS = process_OLS(ols_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_JM_data(folder_path, method):\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    results[\"datasets\"]=[]\n",
    "    for data_file in [i for i in os.listdir(folder_path) if \".pickle\" in i]:\n",
    "        data = pickle.load(open(os.path.join(folder_path, data_file), \"rb\")) \n",
    "        for key in data.keys():\n",
    "            if key==\"results\":\n",
    "                lams = data[key][0][0]\n",
    "                deb_preds = data[key][1][0]\n",
    "                main_preds = data[key][2][0]\n",
    "                \n",
    "                y_tests = data[key][3]\n",
    "                kappas = data[key][4]\n",
    "                feasibles = data[key][5]\n",
    "                dataset = data[\"dataset\"]\n",
    "                \n",
    "                if method==data[\"main_reg_params\"][\"method\"]:\n",
    "                \n",
    "                    i=0\n",
    "                    results[\"lams\"] = lams\n",
    "                    results[\"datasets\"].append(dataset)\n",
    "                    results[dataset] = feasibles[0][0]\n",
    "                    for lam in lams:\n",
    "                        results[(dataset, lam, \"debiased_err\")] = r_err(deb_preds[i]-y_tests)\n",
    "                        results[(dataset, lam, \"all_points\")] = deb_preds[i]-y_tests\n",
    "                        results[(dataset, lam, \"feasible\")] = feasibles[0][i]\n",
    "                        i+=1\n",
    "                    results[(dataset, \"main_err\")] = r_err(main_preds-y_tests)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_dataset(dataset, results):\n",
    "    \n",
    "    for key in data:\n",
    "        if \"main\" in str(key) and dataset in str(key):\n",
    "            main_err = data[key]\n",
    "            \n",
    "    db = []\n",
    "    for key in data:\n",
    "        if \"debiased\" in str(key) and dataset in str(key):\n",
    "            db.append((key[1], data[key]))\n",
    "    return main_err, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_feasible(dataset, results):\n",
    "    \n",
    "    for key in results:\n",
    "        if \"main\" in str(key) and dataset in str(key):\n",
    "            main_err = results[key]\n",
    "            \n",
    "    num_pts=len(results[dataset])\n",
    "    first_feasible = [False for i in range(num_pts)]\n",
    "    datapoint_errs = [None for i in range(num_pts)]\n",
    "    \n",
    "    for key in results:\n",
    "        if \"all_points\" in str(key) and dataset in str(key):\n",
    "            lam = key[1]\n",
    "            for i in range(num_pts):\n",
    "                first = results[(dataset, lam, \"feasible\")][i]\n",
    "                val = results[(dataset, lam, \"all_points\")][0][i]\n",
    "                if first_feasible[i]==False and first==True:\n",
    "                    datapoint_errs[i]=val\n",
    "                    first_feasible[i]=True\n",
    "                \n",
    "                if i==(num_pts-1) and first_feasible[i]==False:\n",
    "                    datapoint_errs[i]=val\n",
    "            \n",
    "    return main_err, np.array(datapoint_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_JM(folder_path):\n",
    "    \n",
    "    JM_stuff= []\n",
    "    computer_cpus=48\n",
    "    main_reg_params = [{\"method\" : \"Lasso\"}, {\"method\" : \"Ridge\"}, {\"method\" : \"Elastic\"}]\n",
    "    for method in main_reg_params:\n",
    "        JM_data = get_JM_data(folder_path = folder_path, method=method[\"method\"])\n",
    "        for dataset in JM_data[\"datasets\"]:\n",
    "            main_err, debiased_errs = min_feasible(dataset, JM_data)\n",
    "            JM_stuff.append(((dataset, \"JM \" +\"(\"+str(method[\"method\"])+\")\"), r_err_sd(debiased_errs)))\n",
    "    \n",
    "    return list(set(JM_stuff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jm_folder_path=\"7-25-JM_Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JM = process_JM(jm_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_OM_data(folder_path):\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    for data_file in [i for i in os.listdir(folder_path) if \".pickle\" in i]:\n",
    "        data = pickle.load(open(os.path.join(folder_path, data_file), \"rb\")) \n",
    "        for key in data.keys():\n",
    "            if key==\"results\":\n",
    "                #print(len(data[key]))\n",
    "                \n",
    "                fo1f=np.array(data[key][0])\n",
    "                foq=np.array(data[key][3])\n",
    "                main=np.array(data[key][6])\n",
    "                truth=np.array(data[key][7])\n",
    "            \n",
    "                errors_fo1f = r_err_sd(fo1f-truth)\n",
    "                errors_foq = r_err_sd(foq-truth)\n",
    "                errors_main = r_err_sd(main-truth)\n",
    "                best_method_f = data[key][-2]\n",
    "                best_method_q = data[key][-1]\n",
    "                \n",
    "                results[(data[\"n_splits\"], data[\"main_reg_params\"][\"method\"], data[\"aux_reg_params\"][\"method\"], data[\"dataset\"],   \"fo1f\")] = errors_fo1f\n",
    "                results[(data[\"n_splits\"], data[\"main_reg_params\"][\"method\"], data[\"aux_reg_params\"][\"method\"], data[\"dataset\"], \"foq\")] = errors_foq\n",
    "                results[(data[\"n_splits\"], data[\"main_reg_params\"][\"method\"], data[\"aux_reg_params\"][\"method\"], data[\"dataset\"], \"baseline\")] = errors_main\n",
    "                results[(data[\"dataset\"], \"aux_choice_f\")] = best_method_f\n",
    "                results[(data[\"dataset\"], \"aux_choice_q\")] = best_method_q\n",
    "            \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "om_folder_path=\"8-1-weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OM_data = get_OM_data(folder_path = om_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OM_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_method(x):\n",
    "    \n",
    "    return x[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_OM(folder_path):\n",
    "    \n",
    "    OM_data = get_OM_data(folder_path = folder_path)\n",
    "    OM = list(OM_data.items())\n",
    "    \n",
    "    stuff=[]\n",
    "    for x in OM:\n",
    "        if x[0][-1] == \"fo1f\":\n",
    "            stuff.append((((x[0][3], \"OM $f$ \" + \"(\" + str(x[0][1]) + \")\"), x[1])))\n",
    "        if x[0][-1] == \"foq\":\n",
    "            stuff.append((((x[0][3], \"OM $q$ \" + \"(\" +str(x[0][1]) + \")\"), x[1])))\n",
    "    for x in OM:\n",
    "        if x[0][-1] == \"baseline\":\n",
    "            stuff.append(((x[0][3], str(x[0][1])), x[1]))\n",
    "\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "om_results = process_OM(om_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "om_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets={\"Fertility\" : [69, 31, 8, \"Yes\"], \"Fire\" : [320, 197, 10, \"Yes\"], \"Parkinson\" : [1877, 3998, 17, \"Yes\"], \"Wine\" : [4898, 1599, 11, \"Yes\"], \"Triazines\" : [139, 47, 60, \"No\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all_results(ols_folder_path, jm_folder_path, om_folder_path):\n",
    "    \n",
    "    OLS = process_OLS(ols_folder_path)\n",
    "    JM = process_JM(jm_folder_path)\n",
    "    OM = process_OM(om_folder_path)\n",
    "    \n",
    "    l = [OLS, JM, OM]\n",
    "    al=[]\n",
    "    for i in l:\n",
    "        for j in i:\n",
    "            al.append(j)\n",
    "            \n",
    "    method = lambda x: x[0][0]\n",
    "    \n",
    "    return sorted(al, key=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols_folder_path = \"7-28-LinReg_Real\"\n",
    "jm_folder_path = \"7-25-JM_Real\"\n",
    "om_folder_path = \"7-24-OM_Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = load_all_results(ols_folder_path, jm_folder_path, om_folder_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_matrix(sizes, data):\n",
    "    \n",
    "    stuff = defaultdict(lambda: defaultdict(list))\n",
    "    for row in data:\n",
    "        dataset = row[0][0]\n",
    "        method = row[0][1]\n",
    "        val = row[1]\n",
    "        el = str(truncate(val[0], dig=4)) + \"$\" + \"\\\\\" + \"pm$\" + str(truncate(truncate(val[0]+val[1], dig=4)-truncate(val[0], dig=4), dig=4))\n",
    "        stuff[dataset][method].append(el)\n",
    "        \n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transpose_matrix(mat):\n",
    "    \n",
    "    stuff = defaultdict(lambda: defaultdict(list))\n",
    "    for dataset in mat:\n",
    "        for method in mat[dataset]:\n",
    "            val = mat[dataset][method][0]\n",
    "            stuff[method][dataset] = val\n",
    "        \n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = make_matrix(datasets, l);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_res = transpose_matrix(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_table_trans(sizes, matrix):\n",
    "    \n",
    "    header = [\"Method\", \"Fertility\", \"Fire\", \"Parkinson\", 'Wine', 'Triazines']\n",
    "    \n",
    "    f = open('real_trans.csv', 'w')\n",
    "    writer = csv.writer(f, delimiter=\",\")\n",
    "    methods = ['OLS', 'Ridge', 'JM (Ridge)', 'OM $f$ (Ridge)', 'OM $q$ (Ridge)',\n",
    "               'Lasso', 'JM (Lasso)', 'OM $f$ (Lasso)', 'OM $q$ (Lasso)',\n",
    "               'Elastic', 'JM (Elastic)', 'OM $f$ (Elastic)', 'OM $q$ (Elastic)'\n",
    "               ]\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    for method in methods:\n",
    "        row=[method]\n",
    "        for dataset in header:\n",
    "            if dataset!=\"Method\":\n",
    "                row.append(matrix[method][dataset])\n",
    "        writer.writerow(row)\n",
    "    f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_table_trans(datasets, trans_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
