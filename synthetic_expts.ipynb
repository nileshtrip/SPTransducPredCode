{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import cvxpy as cvx\n",
    "import mosek\n",
    "import copy\n",
    "\n",
    "import mkl\n",
    "import pickle\n",
    "import os\n",
    "import ray\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import subprocess\n",
    "subprocess.call('bash convert_files.sh', shell=True)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import OrderedDict\n",
    "from numpy import transpose as trans\n",
    "from auxiliary import is_pos_def, cond, rotate_matrix, gen_train_data, gen_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ray.init(object_store_memory=int(7e10), num_cpus=48,  redis_password=\"password54322423\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_lasso(train_data, sigma, cv, fit_intercept, alpha_scaling, n_folds=5):\n",
    "    \n",
    "    \"\"\" Lasso fitter. If cv True uses CV to fit lasso; if false will use alpha_scaling * \\sqrt{2 log p/n} as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Lasso, LassoCV\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling*sigma*math.sqrt(2* (math.log(p)/n))\n",
    "        lasso = Lasso(alpha=alpha, max_iter=5000, fit_intercept=fit_intercept)\n",
    "        lasso.fit(X_train, y_train)\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        lasso=LassoCV(max_iter=5000, cv=n_folds, alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        lasso.fit(X_train, y_train)\n",
    "        \n",
    "    return lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_par_lasso(train_data, sigma, cv, fit_intercept, threads, alpha_scaling, n_folds=5, max_calls=1):\n",
    "\n",
    "    \"\"\" Parallel version of Lasso regression fitter. If cv True uses CV to fit lasso; if false will use alpha_scaling * \\sqrt{2 log p/n} as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "    \n",
    "    mkl.set_num_threads(threads)\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Lasso, LassoCV\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling*sigma*math.sqrt(2* (math.log(p)/n))\n",
    "        lasso = Lasso(alpha=alpha, max_iter=5000, fit_intercept=fit_intercept)\n",
    "        lasso.fit(X_train.copy(), y_train.copy())\n",
    "    else:\n",
    "        alphas = np.logspace(-6, 1, num=100)\n",
    "        lasso=LassoCV(max_iter=5000, cv=n_folds, alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        lasso.fit(X_train.copy(), y_train.copy())\n",
    "        \n",
    "    return lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_ridge(train_data, sigma, cv, fit_intercept, alpha_scaling=1.0):\n",
    "    \n",
    "    \"\"\" ridge regression fitter. If cv True uses CV to fit; if false will use alpha_scaling as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling\n",
    "        ridge = Ridge(alpha=alpha, fit_intercept=fit_intercept)\n",
    "        ridge.fit(X_train, y_train)\n",
    "    else:\n",
    "        alphas = np.logspace(-2, 6, num=100)\n",
    "        ridge=RidgeCV(alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        ridge.fit(X_train, y_train)\n",
    "\n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_par_ridge(train_data, sigma, cv, fit_intercept, threads, alpha_scaling=1.0, max_calls=1):\n",
    "    \n",
    "    \"\"\" Parallel version of ridge regression fitter. If cv True uses CV to fit; if false will use alpha_scaling as a regularizer\n",
    "    fit_intercept determines whether or not to fit the y-intercept in the regression. This set to false by default. \"\"\"\n",
    "    \n",
    "    mkl.set_num_threads(threads) # set number of threads for function.\n",
    "    X_train, y_train = train_data\n",
    "    n, p = X_train.shape\n",
    "\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # Theoretically Optimal regularization and CV regularizers\n",
    "    if not cv:\n",
    "        alpha = alpha_scaling\n",
    "        ridge = Ridge(alpha=alpha, fit_intercept=fit_intercept)\n",
    "        ridge.fit(X_train.copy(), y_train.copy())\n",
    "    else:\n",
    "        alphas = np.logspace(-2, 6, num=100)\n",
    "        ridge=RidgeCV(alphas=alphas, fit_intercept=fit_intercept)\n",
    "        \n",
    "        # Run LassoCV with the metric for CV as MSE\n",
    "        ridge.fit(X_train.copy(), y_train.copy()) #copy data since parallelizing\n",
    "        \n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_f_lassos(train_data, sigma, cv, fit_intercept, n_splits, threads, alpha_scaling, n_folds=5):\n",
    "    \n",
    "    \"\"\"Fit all needed f lasso regressions needed for OM estimator when using n_splits-crossfitting\"\"\"\n",
    "\n",
    "    X, y = train_data\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    lassos=ray.get([fit_par_lasso.remote((X[train_index, :], y[train_index]), sigma, cv, fit_intercept, threads=threads, alpha_scaling=alpha_scaling, n_folds=n_folds) for train_index, test_index in kf.split(X)])\n",
    "    \n",
    "    return lassos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_f_ridges(train_data, sigma, cv, fit_intercept, n_splits, threads, alpha_scaling=1.0):\n",
    "\n",
    "    \"\"\"Fit all needed f ridge regressions needed for OM estimator when using n_splits-crossfitting\"\"\"\n",
    "    \n",
    "    X, y = train_data\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    ridges=ray.get([fit_par_ridge.remote((X[train_index, :], y[train_index]), sigma, cv, fit_intercept, threads=threads, alpha_scaling=alpha_scaling) for train_index, test_index in kf.split(X)])\n",
    "    \n",
    "    return ridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_g_ridge(train_data, test_point, sigma, cv, fit_intercept, threads, n_splits, alpha_scaling, max_calls=1):\n",
    "    \n",
    "    \"\"\" Parallel function to fit ridge estimator for auxiliary g equations. \"\"\"\n",
    "    \n",
    "    mkl.set_num_threads(threads) # Set number of threads.\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X = train_data\n",
    "    kf = KFold(n_splits=n_splits)  # n_split cross fitting.\n",
    "    \n",
    "    # builds (scaled) rotation matrix U\n",
    "    test_norm = np.linalg.norm(test_point, 2)\n",
    "    U = test_norm * rotate_matrix(test_point)\n",
    "    U_inv = np.linalg.inv(U)\n",
    "    \n",
    "    # construct rotated data matrix (which uses U_inv^\\top)\n",
    "    Xrot = X @ U_inv\n",
    "    # split features into \"T\" and remaining \"X\"\n",
    "    t, x = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "    model_ts = [fit_ridge((x[train_index, :], t[train_index]), sigma=sigma, cv=cv, fit_intercept=fit_intercept, alpha_scaling=alpha_scaling) for train_index, test_index in kf.split(X)]\n",
    "    \n",
    "    return model_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_g_ridges(train_data, test_data, sigma, cv, fit_intercept, threads, n_splits, alpha_scaling, n_folds=5):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X_train = train_data\n",
    "    X_test = test_data\n",
    "    num_train, p = X_train.shape\n",
    "    num_test, p = X_test.shape\n",
    "    \n",
    "    stuff = num_test * [None]\n",
    "\n",
    "    id_dict = {fit_g_ridge.remote(train_data=train_data, test_point=X_test[i, :], sigma=sigma, cv=cv, fit_intercept=fit_intercept, threads=threads, n_splits=n_splits, alpha_scaling=alpha_scaling) : i for i in range(num_test)}\n",
    "    ids = list(id_dict.keys())\n",
    "    old_ids = copy.deepcopy(ids)\n",
    "    \n",
    "    while len(ids) > 0:\n",
    "        ready_ids, ids = ray.wait(ids)\n",
    "        for ready_id in ready_ids:\n",
    "            stuff[id_dict[ready_id]] = ray.get(ready_id)\n",
    "    ray.internal.free(old_ids)\n",
    "            \n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fit_g_lasso(train_data, test_point, sigma, cv, fit_intercept, threads, n_splits, alpha_scaling, n_folds=5, max_calls=1):\n",
    "    \n",
    "    \"\"\"Fit lassos for auxiliary g regression \"\"\"\n",
    "    \n",
    "    mkl.set_num_threads(threads)\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X = train_data\n",
    "    kf = KFold(n_splits=n_splits)  \n",
    "    # builds (scaled) rotation matrix U\n",
    "\n",
    "    test_norm = np.linalg.norm(test_point, 2)\n",
    "    U = test_norm * rotate_matrix(test_point)\n",
    "    U_inv = np.linalg.inv(U)\n",
    "    \n",
    "    # construct rotated data matrix (which uses U_inv^\\top)\n",
    "    Xrot = X @ U_inv\n",
    "    # split features into \"T\" and remaining \"X\"\n",
    "    t, x = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "    model_ts = [fit_lasso((x[train_index, :], t[train_index]), sigma=sigma, cv=cv, fit_intercept=fit_intercept, alpha_scaling=alpha_scaling, n_folds=n_folds) for train_index, test_index in kf.split(X)]\n",
    "    \n",
    "    return model_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_g_lassos(train_data, test_data, sigma, cv, fit_intercept, threads, n_splits, alpha_scaling, n_folds=5):\n",
    "    \n",
    "    \"\"\"Fit all g lassos for auxiliary g regression \"\"\"\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X_train = train_data\n",
    "    X_test = test_data\n",
    "    num_train, p = X_train.shape\n",
    "    num_test, p = X_test.shape\n",
    "    \n",
    "    stuff = num_test * [None]\n",
    "\n",
    "    id_dict = {fit_g_lasso.remote(train_data=train_data, test_point=X_test[i, :], sigma=sigma, cv=cv, fit_intercept=fit_intercept, threads=threads, n_splits=n_splits, alpha_scaling=alpha_scaling) : i for i in range(num_test)}\n",
    "    ids = list(id_dict.keys())\n",
    "    old_ids = copy.deepcopy(ids)\n",
    "    \n",
    "    while len(ids) > 0:\n",
    "        ready_ids, ids = ray.wait(ids)\n",
    "        for ready_id in ready_ids:\n",
    "            stuff[id_dict[ready_id]] = ray.get(ready_id)\n",
    "    ray.internal.free(old_ids)\n",
    "            \n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote \n",
    "def fit_f_moms_1f(data, sigma, test_point, g_bases, f_bases, cv, fit_intercept, n_splits, threads, max_calls=1):\n",
    "    \n",
    "    \"\"\"Parallel function to predict y values using the OM f moments. Makes a single prediction on test_point x_* using OM \n",
    "    by rotating all data into the basis where test_point points along e_1. \n",
    "    \n",
    "    Uses (y-\\theta t - z^\\top f) (t-g(z)) moments. Here f(z) is estimated by regressing y on (t,z) and dropping t.\n",
    "    \n",
    "    # data is training data\n",
    "    # test_point is the prediction direction\n",
    "    # sigma is the additive noise\n",
    "    # alpha1 and alpha2 represent the regularization scaling for both lasso regressions respectively needed for prediction\n",
    "    # cv indicates whether or not to use CV\n",
    "    # fit_intercept indicates whether or not to fit y-intercepts\n",
    "    \"\"\"\n",
    "    mkl.set_num_threads(threads) # set number of threads for function\n",
    "\n",
    "    # builds (scaled) rotation matrix U\n",
    "    X, y = data\n",
    "    test_norm = np.linalg.norm(test_point, 2)\n",
    "    U = test_norm * rotate_matrix(test_point)\n",
    "    U_inv = np.linalg.inv(U)\n",
    "    \n",
    "    # construct rotated data matrix (which uses U_inv^\\top)\n",
    "    Xrot = X @ U_inv\n",
    "    # split features into \"T\" and remaining \"X\"\n",
    "    Trot, Xrot = Xrot[:, 0], Xrot[:, 1:]\n",
    "\n",
    "    # arrays to contain p and q residuals from regression \n",
    "    res_t = np.zeros(X.shape[0])\n",
    "    res_y = np.zeros(X.shape[0])\n",
    "\n",
    "    kf = KFold(n_splits=n_splits) #first stage CF\n",
    "    \n",
    "    count=0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data in half, train and test\n",
    "        x_train, t_train, y_train = Xrot[train_index, :], Trot[train_index], y[train_index]\n",
    "        x_test, t_test, y_test  = Xrot[test_index, :], Trot[test_index], y[test_index]\n",
    "        \n",
    "        # Fit Lasso models for \"y\" in original basis\n",
    "        model_y_orig = f_bases[count]\n",
    "        y_coef = model_y_orig.coef_\n",
    "        y_int = model_y_orig.intercept_\n",
    "        \n",
    "        # Rotate \"y\" coefficients (i.e. f) into new basis and \"drop\" first t coefficient\n",
    "        y_coef_rot = (U @ y_coef)[1:]\n",
    "\n",
    "        model_t = g_bases[count]\n",
    "        res_t[test_index] = (t_test - model_t.predict(x_test)).flatten()\n",
    "        res_y[test_index] = (y_test - (x_test @ y_coef_rot + y_int)).flatten()\n",
    "        count+=1\n",
    "\n",
    "\n",
    "    num1 = np.mean(res_y * res_t)\n",
    "    denom1 = np.mean(Trot * res_t)\n",
    "    pred1 = num1 / denom1\n",
    "    \n",
    "    return pred1, num1, denom1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_all_f_moms_1f(train_data, X_test, g_bases, f_bases, sigma, cv, fit_intercept, n_splits, threads):\n",
    "\n",
    "    \"\"\" Make predictions over entire test set in X_test using f_lassos, g_lasso which have already been fit. \"\"\"\n",
    "    \n",
    "    X_train, y_train = train_data \n",
    "    _, p = X_train.shape\n",
    "    \n",
    "    num_test,_ = X_test.shape\n",
    "    stuff = num_test * [None]\n",
    "    \n",
    "    # Function to make predictions over test set in parallel\n",
    "    id_dict = {fit_f_moms_1f.remote(train_data,  sigma, X_test[i, :], g_bases[i], f_bases, cv=cv, fit_intercept=fit_intercept, n_splits=n_splits, threads=threads) : i for i in range(num_test)}\n",
    "    ids = list(id_dict.keys())\n",
    "    old_ids = copy.deepcopy(ids)\n",
    "    \n",
    "    while len(ids) > 0:\n",
    "        ready_ids, ids = ray.wait(ids)\n",
    "        for ready_id in ready_ids:\n",
    "            stuff[id_dict[ready_id]] = ray.get(ready_id)\n",
    "    \n",
    "    pred1s, num1s, denom1s = zip(*stuff)\n",
    "    ray.internal.free(old_ids)\n",
    "\n",
    "    return (np.array(pred1s), np.array(num1s), np.array(denom1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_OM_expt(p, train_n, test_n, s, train_dist, test_dist, cv, fit_intercepts, n_splits, x_scale_test, beta_scale, sigma, alpha_scalings, main_reg_params, aux_reg_params, seed, threads):\n",
    "    \"\"\"Runs an experiment for a single problem instance with given values of p, n, s.\"\"\"\n",
    "\n",
    "    # Options to specify whether intercepts should be fit and the scaling of the regularizers\n",
    "    fit_intercept_main, fit_intercept_g = fit_intercepts\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    alpha1, alpha2 = alpha_scalings\n",
    "    \n",
    "    # Generate training and test data\n",
    "    np.random.seed(seed)\n",
    "    X_train, y_train, coef, scaler, kappa = gen_train_data(n=train_n, p=p, s=s, train_dist=train_dist, x_scale=1.0, beta_scale=beta_scale, sigma=sigma)\n",
    "    X_test, y_test = gen_test_data(test_n, p, s, coef, X_scaler=scaler, test_dist=test_dist, x_scale=x_scale_test, sigma=0*sigma)\n",
    "    assert(fit_intercept_main==False, \"This should be set to false for all f regression. g_lasso has been manually set to include this\")\n",
    "    \n",
    "    # Demean y values\n",
    "    mu_y = np.mean(y_train)\n",
    "    y_train = y_train - mu_y\n",
    "    \n",
    "    # Parameters for Main (f) and Auxiliary (g) regressions.\n",
    "    aux_reg = aux_reg_params[\"method\"]\n",
    "    main_reg = main_reg_params[\"method\"]\n",
    "    \n",
    "    if main_reg==\"Lasso\":\n",
    "        f_base = fit_all_f_lassos((X_train, y_train), sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads, alpha_scaling=alpha1, n_folds=5)\n",
    "    elif main_reg==\"Ridge\":\n",
    "        f_base = fit_all_f_ridges((X_train, y_train), sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads, alpha_scaling=alpha1)\n",
    "    \n",
    "    aux_reg = aux_reg_params[\"method\"]\n",
    "    if aux_reg==\"Lasso\":\n",
    "        g_base = fit_all_g_lassos(X_train, test_data=X_test, sigma=sigma, cv=cv, fit_intercept=fit_intercept_g, n_splits=n_splits, threads=threads, alpha_scaling=alpha2, n_folds=5)\n",
    "    if aux_reg==\"Ridge\":\n",
    "        g_base = fit_all_g_ridges(X_train, test_data=X_test, sigma=sigma, cv=cv, fit_intercept=fit_intercept_g, n_splits=n_splits, threads=threads, alpha_scaling=alpha2)\n",
    "    \n",
    "    fo1f = fit_all_f_moms_1f(train_data=(X_train, y_train),  X_test=X_test, g_bases=g_base, f_bases=f_base, sigma=sigma, cv=cv, fit_intercept=fit_intercept_main, n_splits=n_splits, threads=threads)\n",
    "    \n",
    "    \n",
    "    if main_reg==\"Lasso\":\n",
    "        base = fit_lasso((X_train, y_train), sigma, alpha_scaling=alpha1, cv=cv, fit_intercept=fit_intercept_main, n_folds=5)\n",
    "        base_preds = base.predict(X_test)\n",
    "    elif main_reg==\"Ridge\":\n",
    "        base = fit_ridge((X_train, y_train), sigma, alpha_scaling=alpha1, cv=cv, fit_intercept=fit_intercept_main)\n",
    "        base_preds = base.predict(X_test)\n",
    "    \n",
    "    kappa=cond(X_train)\n",
    "    \n",
    "    return fo1f, base_preds, y_test, kappa, mu_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_OM_expts(p_list, n_list, s_list, train_dist, test_dist, cv, fit_intercepts, n_splits, x_scale_test, beta_scale, sigma, alpha_scalings, main_reg_params, aux_reg_params, seeds, threads, reps, folder_path):\n",
    "\n",
    "    \"\"\"Runs multiple trials of an OM experiments for various values of p, n, s and saves that data in pkl file\"\"\"\n",
    "    save_data = OrderedDict()\n",
    "    \n",
    "    fit_intercept_main, fit_intercept_g = fit_intercepts\n",
    "    save_data[\"train_dist\"]=str(train_dist)\n",
    "    save_data[\"test_dist\"]=str(test_dist)\n",
    "    save_data[\"method\"]=\"f moments\"\n",
    "    save_data[\"main_reg\"]=\"CV\"+str(cv)\n",
    "    save_data[\"fit_intercepts\"]=str(fit_intercepts)\n",
    "    save_data[\"format_keys\"] = \"p,n,s\"\n",
    "    save_data[\"output\"] = \"fo1f, lasso_preds, y_test, kappa, mu_y\" + \";\" + \"preds, num, denom\"\n",
    "    save_data[\"n_splits\"] = n_splits\n",
    "    save_data[\"aux_reg_params\"] = str(aux_reg_params)\n",
    "    save_data[\"test_n\"] = str(n_list[0][1])\n",
    "    save_data[\"seeds\"] = seeds\n",
    "    save_data[\"scale_X\"] = True\n",
    "    save_data[\"x_scale_test\"] = x_scale_test\n",
    "    save_data[\"beta_scale\"] = beta_scale\n",
    "    save_data[\"sigma\"] = sigma\n",
    "    save_data[\"alpha_scalings\"] = alpha_scalings[0]\n",
    "    save_data[\"reps\"] = reps\n",
    "    save_data[\"base_regression\"] = main_reg_params[\"method\"]\n",
    "\n",
    "    save_data[\"p_list\"] = p_list\n",
    "    save_data[\"n_list\"] = n_list\n",
    "    save_data[\"s_list\"] = s_list\n",
    "    \n",
    "    print(\"Starting\")\n",
    "    for p in p_list:\n",
    "        for train_n, test_n in n_list:\n",
    "            for s in s_list:\n",
    "                OML_fo1f_preds=[]\n",
    "                OML_fo1f_nums=[]\n",
    "                OML_fo1f_denoms=[]\n",
    "\n",
    "                base_preds=[]\n",
    "                y_tests=[] \n",
    "                kappas=[]\n",
    "                print(str(p)+\",\"+str(train_n)+\",\"+str(s))\n",
    "                    \n",
    "                for rep in range(reps):\n",
    "                    fo1f, base_pred, y_test, kappa, mu_y = run_OM_expt(p, train_n, test_n, s, train_dist=train_dist, test_dist=test_dist, cv=cv, fit_intercepts=fit_intercepts, n_splits=n_splits, x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, alpha_scalings=alpha_scalings[1], main_reg_params=main_reg_params, \n",
    "                                                                               aux_reg_params=aux_reg_params, threads=threads, seed=seeds[rep])\n",
    "                    OML_fo1f_preds.append(fo1f[0]+mu_y)\n",
    "                    OML_fo1f_nums.append(fo1f[1])\n",
    "                    OML_fo1f_denoms.append(fo1f[2])\n",
    "\n",
    "                    base_preds.append(base_pred+mu_y)\n",
    "                    y_tests.append(y_test)\n",
    "                    kappas.append(kappa)\n",
    "\n",
    "                    save_data[(p,train_n,s)] = [\n",
    "                            OML_fo1f_preds,\n",
    "                            OML_fo1f_nums,\n",
    "                            OML_fo1f_denoms,\n",
    "                            base_preds,\n",
    "                            y_tests,\n",
    "                            kappas]\n",
    "\n",
    "    print(\"Saving Data\")\n",
    "    time.sleep(1)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    file_name = \"OM\"+\"_\"+timestr+\".pickle\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    pickle.dump(save_data, open(file_path, \"wb\"))\n",
    "    \n",
    "    return save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def invert(X, x_star, lam, threads=1):\n",
    "\n",
    "    \"\"\"Parallel Function to solve JM program\"\"\"\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    mkl.set_num_threads(threads)\n",
    "    try:\n",
    "        n, p = X.shape\n",
    "        eps = 1e-12\n",
    "        Sigma_n = 1/float(n) * np.transpose(X) @ X  + eps*np.eye(p)\n",
    "        #adding epsilon to make matrix strictly p.s.d. or else cvxpy throws not DCP error (cannot recognize convexity of objective)\n",
    "\n",
    "        w = cvx.Variable(p)\n",
    "        obj = cvx.Minimize(cvx.quad_form(w, Sigma_n))\n",
    "        const = [cvx.norm(Sigma_n * w  - x_star, \"inf\") <= lam]\n",
    "\n",
    "        prob = cvx.Problem(obj, const)\n",
    "        sol = prob.solve(solver=cvx.MOSEK, mosek_params={mosek.iparam.num_threads: threads})\n",
    "        \n",
    "        return sol, w.value\n",
    "    except:\n",
    "        # If solver fails simply return None value for w\n",
    "        return math.inf, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ws(X_train, X_test, lams, threads):\n",
    "    \n",
    "    \"\"\" Computes the entire set of w's needed for X_test using X_train \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    train_n, p = X_train.shape\n",
    "    test_n, p = X_test.shape\n",
    "\n",
    "    ids=[]\n",
    "    lam_deb, method = lams[0], lams[1]\n",
    "    for i in range(test_n):\n",
    "        x_star = X_test[i, :]\n",
    "        x_norm = np.linalg.norm(x_star, ord=2)\n",
    "        \n",
    "        # Computes Debiasing Correction using Theoretical Value of \\lambda_w\n",
    "        if method==\"theory\":\n",
    "            if (train_n >= 1.5*p):\n",
    "                lam_deb = .01*lam_deb\n",
    "                ids.append(invert.remote(X_train, x_star, lam=lam_deb*x_norm, threads=threads))\n",
    "            else:\n",
    "                ids.append(invert.remote(X_train, x_star, lam=lam_deb*x_norm, threads=threads))\n",
    "        # Computes Debiasing Correction for a fixed \\lambda_w\n",
    "        elif method==\"grid\":\n",
    "            ids.append(invert.remote(X_train, x_star, lam=lam_deb*x_norm, threads=threads))\n",
    "        else:\n",
    "            raise Exception(\"Aux Debiasing Set Incorrectly\")\n",
    "    vals = ray.get(ids) # Get values from Ray\n",
    "    ray.internal.free(ids) \n",
    "    \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debias_base(train_data, base_model, X_test, ws):\n",
    "    \n",
    "    \"\"\"Computes JM Debiased Predictions with respect to the base model using base_model and learned ws\"\"\"\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X_train, y_train = train_data\n",
    "    train_n, p = X_train.shape\n",
    "    test_n, p = X_test.shape\n",
    "    \n",
    "    beta = base_model.coef_\n",
    "    beta_int = base_model.intercept_\n",
    "\n",
    "    y_preds=np.zeros(test_n)\n",
    "    resid = np.transpose(X_train) @ (y_train-X_train @ beta-beta_int)\n",
    "    feasible=[]\n",
    "    for i in range(test_n):\n",
    "        x_star = X_test[i, :]\n",
    "        #Lasso Prediction\n",
    "        y_preds[i] = x_star.dot(beta) + beta_int\n",
    "        val, w = ws[i]\n",
    "        #debiasing correction\n",
    "        if val==math.inf:\n",
    "            feasible.append(False)\n",
    "        else:\n",
    "            y_preds[i] += 1/float(train_n) * np.ravel(w).dot(resid)\n",
    "            feasible.append(True)\n",
    "    \n",
    "    return y_preds, feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_JM_expt(p, main_reg_params, train_n, test_n, s, train_dist, test_dist, cv, fit_intercept, alpha_scaling, x_scale_test, beta_scale, sigma, lams, threads, seed):\n",
    "    \n",
    "    \"\"\"Runs a JM experiment for a single problem instance with given values of p, n, s.\"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    # Generate training and test data\n",
    "    X_train, y_train, coef, scaler, kappa = gen_train_data(n=train_n, p=p, s=s, train_dist=train_dist, x_scale=1.0, beta_scale=beta_scale, sigma=sigma)\n",
    "    X_test, y_test = gen_test_data(test_n, p, s, coef, X_scaler=scaler, test_dist=test_dist, x_scale=x_scale_test, sigma=0*sigma)\n",
    "    assert(fit_intercept==False, \"This should be set to false for all f regression. g_lasso has been manually set to include this\")\n",
    "    \n",
    "    # Demean y values\n",
    "    mu_y = np.mean(y_train)\n",
    "    y_train = y_train - mu_y\n",
    "    \n",
    "    # Parameters for Main (f) and Auxiliary (g) regressions.\n",
    "    main_reg = main_reg_params[\"method\"]\n",
    "    \n",
    "    # Fit Main Base Regression\n",
    "    if main_reg==\"Lasso\":\n",
    "        base = fit_lasso((X_train, y_train), sigma=sigma, cv=cv, fit_intercept=fit_intercept, alpha_scaling=alpha_scaling, n_folds=5)\n",
    "        base_preds = base.predict(X_test)\n",
    "    elif main_reg==\"Ridge\":\n",
    "        base = fit_ridge((X_train, y_train), sigma=sigma, cv=cv, fit_intercept=fit_intercept, alpha_scaling=alpha_scaling)\n",
    "        base_preds = base.predict(X_test)\n",
    "        \n",
    "    # Compute Debiased Predictions Either Using Theoretical Values or Entire Grid\n",
    "    lam_vals, method = lams[0], lams[1]\n",
    "    if method==\"theory\":\n",
    "        lam_deb = 1.5*math.sqrt(math.log(p)/train_n)\n",
    "        ws = compute_ws(X_train, X_test, lams=(lam_deb, method), threads=threads)\n",
    "        deb_pred, feasible = debias_base((X_train, y_train), base, X_test, ws)\n",
    "        deb_preds = [deb_pred]\n",
    "        feasibles = [feasible]\n",
    "        ws = [ws]\n",
    "    elif method==\"grid\":\n",
    "        deb_preds=[]\n",
    "        feasibles=[]\n",
    "        ws=[]\n",
    "        for lam_deb in lam_vals:\n",
    "            w = compute_ws(X_train, X_test, lams=(lam_deb, method), threads=threads)\n",
    "            deb_pred, feasible = debias_base((X_train, y_train), base, X_test, w)\n",
    "            deb_preds.append(deb_pred)\n",
    "            feasibles.append(feasible)\n",
    "        \n",
    "    return deb_preds, base_preds, ws, y_test, kappa, feasibles, mu_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_JM_expts(p_list, n_list, s_list, main_reg_params, train_dist, test_dist, cv, fit_intercepts, alpha_scalings, x_scale_test, beta_scale, sigma, lams, seeds, threads, reps, folder_path):\n",
    "\n",
    "    \"\"\"Runs multiple trials of an JM experiments for various values of p, n, s and saves that data in pkl file\"\"\"\n",
    "    \n",
    "    save_data = OrderedDict()\n",
    "    \n",
    "    fit_intercept_main, _ = fit_intercepts\n",
    "    save_data[\"train_dist\"]=str(train_dist)\n",
    "    save_data[\"test_dist\"]=str(test_dist)\n",
    "    save_data[\"method\"]=\"JM Debiased Lasso\"\n",
    "    save_data[\"main_reg\"]=\"CV\"+str(cv)\n",
    "    save_data[\"fit_intercepts\"]=str(fit_intercepts)\n",
    "    save_data[\"format_keys\"] = \"p,n,s\"\n",
    "    save_data[\"output\"] = \"deb_preds, lasso_preds, ws, y_test, kappa\"\n",
    "    save_data[\"test_n\"] = str(n_list[0][1])\n",
    "    save_data[\"seeds\"] = seeds\n",
    "    save_data[\"scale_X\"] = True\n",
    "    save_data[\"x_scale_test\"] = x_scale_test\n",
    "    save_data[\"beta_scale\"] = beta_scale\n",
    "    save_data[\"sigma\"] = sigma\n",
    "    save_data[\"alpha_scalings\"] = alpha_scalings[0]\n",
    "    save_data[\"reps\"] = reps\n",
    "    save_data[\"lam\"] = lams[0]\n",
    "    save_data[\"lam_method\"] = lams[1]\n",
    "    save_data[\"base_regression\"] = main_reg_params\n",
    "    \n",
    "    save_data[\"p_list\"] = p_list\n",
    "    save_data[\"n_list\"] = n_list\n",
    "    save_data[\"s_list\"] = s_list\n",
    "\n",
    "    print(\"Starting\")\n",
    "    for p in p_list:\n",
    "        for train_n, test_n in n_list:\n",
    "            for s in s_list:\n",
    "                deb_preds=[]\n",
    "                base_preds=[]\n",
    "                y_tests=[] \n",
    "                kappas=[]\n",
    "                feasibles=[]\n",
    "                regs=[]\n",
    "                if (s <= p and float(s)/p >= 0.1):\n",
    "                    print(str(p)+\",\"+str(train_n)+\",\"+str(s))\n",
    "                    for rep in range(reps):\n",
    "                        deb_pred, base_pred, w, y_test, kappa, feasible, mu_y = run_JM_expt(p=p, main_reg_params=main_reg_params, train_n=train_n, test_n=test_n, s=s, train_dist=train_dist, test_dist=test_dist, cv=cv, fit_intercept=fit_intercept_main, alpha_scaling=alpha_scalings[1], x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, lams=lams, threads=threads, seed=seeds[rep])\n",
    "                        deb_preds.append([i+mu_y for i in deb_pred])\n",
    "                        base_preds.append(base_pred+mu_y)\n",
    "                        y_tests.append(y_test)\n",
    "                        kappas.append(kappa)\n",
    "                        feasibles.append(feasible)\n",
    "                save_data[(p,train_n,s)] = [\n",
    "                        deb_preds,\n",
    "                        base_preds,\n",
    "                        y_tests,\n",
    "                        kappas,\n",
    "                        feasibles]\n",
    "\n",
    "    print(\"Saving Data\")\n",
    "    time.sleep(1)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    file_name = \"JM\"+\"_\"+timestr+\".pickle\"\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    pickle.dump(save_data, open(file_path, \"wb\"))\n",
    "    \n",
    "    return save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to set up Experiment to Reproduce Synthetic Experiments for Lasso Debiasing involving both fixed and cross-validated hyperparameters for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_list = [200]\n",
    "n_list = [item for item in itertools.product([50, 100, 200, 400, 800, 1600, 3200], [500])]\n",
    "s_list = [20, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dists=[(\"normal\", {}), (\"normal\", {}), (\"normal\", {})]\n",
    "test_dists=[(\"normal\", {}), (\"normal+support_shift\", {\"scale\" : 10.0}), (\"normal+support_var\", {\"support_scale\" : 10.0})]\n",
    "\n",
    "dists = list(zip(train_dists, test_dists))\n",
    "scaling_params = [(1.0, 1.0, 1.0)]\n",
    "alpha_scalings_JM = [4.0]\n",
    "alpha_scalings_OML = [(4.0, 4.0)]\n",
    "\n",
    "OML_aux_regs=[\"Lasso\"]\n",
    "JM_aux_params=[(0, \"theory\")]\n",
    "                \n",
    "splits = [5]\n",
    "fit_intercepts=(False, False)\n",
    "\n",
    "path_options=\"synthetic_lasso_expts\"\n",
    "reps=20\n",
    "seeds=[100+i for i in range(reps)]\n",
    "\n",
    "main_regs=[{\"method\": \"Lasso\"}]\n",
    "aux_regs=[{\"method\": \"Lasso\"}]\n",
    "\n",
    "threads=1 #Should be to the number of threads desired for each regression\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "folder_path=str(now.month)+\"-\"+str(now.day)+\"-\"+path_options\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for split in splits:\n",
    "        for aux_reg in aux_regs:\n",
    "            for i, (alpha_scalings, (x_scale_test, beta_scale, sigma)) in enumerate(zip(alpha_scalings_OML, scaling_params)):\n",
    "                total+=1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for n_split in splits:\n",
    "        for main_reg_params in main_regs:\n",
    "            for aux_reg_params in aux_regs:\n",
    "                for i, (alpha_scalings, (x_scale_test, beta_scale, sigma)) in enumerate(zip(alpha_scalings_OML, scaling_params)):\n",
    "                    count+=1\n",
    "                    print(\"{0:.0%}\".format(float(count)/total)+\" Done\")\n",
    "                    data_cv_false_OM = save_OM_expts(p_list, n_list, s_list, train_dist=train_dist, test_dist=test_dist, cv=False, fit_intercepts=fit_intercepts, n_splits=n_split, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, main_reg_params=main_reg_params, aux_reg_params=aux_reg_params, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)\n",
    "                    data_cv_true_OM = save_OM_expts(p_list, n_list, s_list, train_dist=train_dist, test_dist=test_dist, cv=True, fit_intercepts=fit_intercepts, n_splits=n_split, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, main_reg_params=main_reg_params, aux_reg_params=aux_reg_params, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for (alpha_scalings, (x_scale_test, beta_scale, sigma)) in zip(alpha_scalings_JM, scaling_params):\n",
    "        for lams in JM_aux_params:\n",
    "            total+=1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for main_reg_params in main_regs:\n",
    "        for i, (alpha_scalings, (x_scale_test, beta_scale, sigma)) in enumerate(zip(alpha_scalings_JM, scaling_params)):\n",
    "            for lams in JM_aux_params:\n",
    "                count+=1\n",
    "                print(\"{0:.0%}\".format(float(count)/total)+\" Done\")\n",
    "                data_cv_false_JM = save_JM_expts(p_list, n_list, s_list, main_reg_params=main_reg_params, train_dist=train_dist, test_dist=test_dist, cv=False, fit_intercepts=fit_intercepts, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, lams=lams, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)\n",
    "                data_cv_true_JM = save_JM_expts(p_list, n_list, s_list, main_reg_params=main_reg_params, train_dist=train_dist, test_dist=test_dist, cv=True, fit_intercepts=fit_intercepts, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, lams=lams, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to set up Experiment to Reproduce Synthetic Experiments for Ridge Regression Debiasing involving both fixed and cross-validated hyperparameters for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_list = [200]\n",
    "n_list = [item for item in itertools.product([50, 100, 200, 400, 800, 1600, 3200], [500])]\n",
    "s_list = [200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dists=[(\"normal\", {}), (\"normal\", {})]\n",
    "test_dists=[(\"normal+support_shift\", {\"scale\" : 10.0}), (\"normal+support_rank_one\", {\"support_scale\" : 10.0})]\n",
    "\n",
    "dists = list(zip(train_dists, test_dists))\n",
    "p=p_list[0]\n",
    "scaling_params = [(1.0, 1.0/math.sqrt(p), 1.0)]\n",
    "\n",
    "alpha_scalings_JM = [p/(p*i[1]**2) for i in scaling_params]\n",
    "alpha_scalings_OM = [(p/(p*i[1]**2), 1e+12) for i in scaling_params]\n",
    "OML_aux_regs=[\"Ridge\"]\n",
    "JM_aux_params=[(0, \"theory\")]\n",
    "\n",
    "splits = [5]\n",
    "fit_intercepts=(False, False)\n",
    "\n",
    "path_options=\"synthetic_ridge_expts\"\n",
    "reps=20\n",
    "seeds=[100+i for i in range(reps)]\n",
    "\n",
    "main_regs=[{\"method\": \"Ridge\"}]\n",
    "aux_regs=[{\"method\": \"Ridge\"}]\n",
    "threads=8 #Should be to the number of threads desired for each regression\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "folder_path=str(now.month)+\"-\"+str(now.day)+\"-\"+path_options\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for split in splits:\n",
    "        for aux_reg in aux_regs:\n",
    "            for i, (alpha_scalings, (x_scale_test, beta_scale, sigma)) in enumerate(zip(alpha_scalings_OM, scaling_params)):\n",
    "                total+=1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for n_split in splits:\n",
    "        for main_reg_params in main_regs:\n",
    "            for aux_reg_params in aux_regs:\n",
    "                for i, (alpha_scalings, (x_scale_test, beta_scale, sigma)) in enumerate(zip(alpha_scalings_OM, scaling_params)):\n",
    "                    count+=1\n",
    "                    print(\"{0:.0%}\".format(float(count)/total)+\" Done\")\n",
    "                    data_cv_false_OM = save_OM_expts(p_list, n_list, s_list, train_dist=train_dist, test_dist=test_dist, cv=False, fit_intercepts=fit_intercepts, n_splits=n_split, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, main_reg_params=main_reg_params, aux_reg_params=aux_reg_params, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)\n",
    "                    data_cv_true_OM = save_OM_expts(p_list, n_list, s_list, train_dist=train_dist, test_dist=test_dist, cv=True, fit_intercepts=fit_intercepts, n_splits=n_split, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, main_reg_params=main_reg_params, aux_reg_params=aux_reg_params, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for (alpha_scalings, (x_scale_test, beta_scale, sigma)) in zip(alpha_scalings_JM, scaling_params):\n",
    "        for lams in JM_aux_params:\n",
    "            total+=1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for (train_dist, test_dist) in dists:\n",
    "    for main_reg_params in main_regs:\n",
    "        for i, (alpha_scalings, (x_scale_test, beta_scale, sigma)) in enumerate(zip(alpha_scalings_JM, scaling_params)):\n",
    "            for lams in JM_aux_params:\n",
    "                count+=1\n",
    "                print(\"{0:.0%}\".format(float(count)/total)+\" Done\")\n",
    "                data_cv_false_JM = save_JM_expts(p_list, n_list, s_list, main_reg_params=main_reg_params, train_dist=train_dist, test_dist=test_dist, cv=False, fit_intercepts=fit_intercepts, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, lams=lams, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)\n",
    "                data_cv_true_JM = save_JM_expts(p_list, n_list, s_list, main_reg_params=main_reg_params, train_dist=train_dist, test_dist=test_dist, cv=True, fit_intercepts=fit_intercepts, alpha_scalings=(i, alpha_scalings), x_scale_test=x_scale_test, beta_scale=beta_scale, sigma=sigma, lams=lams, seeds=seeds, threads=threads, reps=reps, folder_path=folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
